{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cliff\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\cliff\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import islice\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "%matplotlib inline\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.cluster import KMeansClusterer\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "nltk.download('punkt')\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some predefinitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best to pre-compile regexes that will be re-used often"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "re_url = re.compile(r'https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+')\n",
    "re_hashtag = re.compile(r'#[^\\s<>\"]+')\n",
    "re_mention = re.compile(r'@[^\\s<>\"]+')\n",
    "tokenizer = RegexpTokenizer(r'\\w+[\\'‘’]?\\w?') #modified to correctly extract \"wouldn't\" etc.\n",
    "stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "657307\n"
     ]
    }
   ],
   "source": [
    "JSONlength = 0\n",
    "with open('geotagged_tweets_20160812-0912.jsons') as myfile:\n",
    "    for line in myfile:\n",
    "        JSONlength += 1\n",
    "print(JSONlength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape US States"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check length of json contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = \"https://simple.wikipedia.org/wiki/List_of_U.S._states\"\n",
    "response = urlopen(url)\n",
    "soup = BeautifulSoup(response, 'html.parser')\n",
    "table = soup.find(\"table\",{\"class\":\"wikitable\"})\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "stateslong = []\n",
    "statesshort = []\n",
    "\n",
    "for row in rows:\n",
    "    cells = row.find_all('td')\n",
    "    if len(cells) > 0:\n",
    "        statesshort.append(cells[1].text.replace(\"\\n\",\"\"))\n",
    "        stateslong.append(cells[2].text.replace(\"\\n\",\"\"))\n",
    "        \n",
    "stateslongnocaps = list(map(str.lower, stateslong))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load file a fill dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#init dataframe\n",
    "df = pd.DataFrame(0, index=np.arange(JSONlength), columns=['lang', 'country', 'state', 'city', 'text', 'mentions'], dtype = 'str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n"
     ]
    }
   ],
   "source": [
    "with open('geotagged_tweets_20160812-0912.jsons') as myfile:\n",
    "    for i, line in enumerate(myfile):\n",
    "        try:\n",
    "            line = json.loads(line)\n",
    "        except Exception as e:\n",
    "            print (e)\n",
    "            continue\n",
    "        \n",
    "        # get row to fill\n",
    "        a = df.iloc[i]\n",
    "        \n",
    "        # extract tweet language\n",
    "        a['lang'] = line['lang'] \n",
    "        \n",
    "        # extract country code\n",
    "        try:\n",
    "            a['country'] = line['place']['country_code']\n",
    "        except Exception as e:\n",
    "            print (e)\n",
    "            continue\n",
    "        \n",
    "        # extract US states if possible\n",
    "        try:\n",
    "            if(a['country'] == 'US'):\n",
    "                #use strip to remove front/back spaces\n",
    "                a['city'], a['state'] = list(map(str.strip, line['place']['full_name'].split(',')))\n",
    "                # checking against USA as state\n",
    "                if a['state'] == \"USA\":\n",
    "                    a['state'] = None\n",
    "                # checking against state as city\n",
    "                if a['city'].lower() in stateslongnocaps:\n",
    "                    stateIndex = stateslongnocaps.index(a['city'].lower())\n",
    "                    a['state'] = statesshort[stateIndex]\n",
    "                    a['city'] = None\n",
    "                elif a['city'] in statesshort:\n",
    "                    a['state'] = a['city']\n",
    "                    a['city'] = None\n",
    "            else:\n",
    "                a['state'], a['city'] = [None, None]\n",
    "        except Exception as e:\n",
    "                # we are now losing many cases of different formats\n",
    "                # I'll write another regex to pick up some more\n",
    "                # - Floris\n",
    "                continue\n",
    "        \n",
    "        mentions = line['entities']['user_mentions']\n",
    "        mentionlist = []\n",
    "        for item in mentions:\n",
    "            mentionlist.append(item['name'])\n",
    "            \n",
    "        a['mentions'] = mentionlist\n",
    "            \n",
    "        a['text'] = line['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define tweet text pre-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_links(text):\n",
    "    match = re_url.findall(text)\n",
    "    if match:\n",
    "        return match\n",
    "    return ''\n",
    "\n",
    "def extract_hashtags(text):\n",
    "    match = re_hashtag.findall(text)\n",
    "    if match:\n",
    "        return match\n",
    "    return ''\n",
    "\n",
    "def remove_links(text):\n",
    "    return re_url.sub(\"\",text)\n",
    "\n",
    "def remove_hashtag_mentions(text):\n",
    "    text = re_hashtag.sub(\"\",text)\n",
    "    return re_mention.sub(\"\",text)\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    #remove stop words\n",
    "    return [token for token in tokens if token not in stopWords]\n",
    "    \n",
    "def lemmatize(tokens):\n",
    "    return [wordnet_lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "def get_tokens(text):\n",
    "    text = remove_links(text)\n",
    "    text = remove_hashtag_mentions(text)\n",
    "    tokens = tokenize(text)\n",
    "    #lemmas = lemmatize(tokens) #skip for now\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def raw_tokens(text):\n",
    "    text = remove_links(text)\n",
    "    text = remove_hashtag_mentions(text)\n",
    "    text = tokenize(text)\n",
    "    tokens = lemmatize(text)\n",
    "    return [token for token in tokens if token not in stopWords]\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['links'] = df['text'].apply(lambda text: extract_links(text))\n",
    "\n",
    "df['hashtags'] = df['text'].apply(lambda text: extract_hashtags(text))\n",
    "\n",
    "df['relevant_tokens'] = df['text'].apply(lambda text: get_tokens(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>state</th>\n",
       "      <th>text</th>\n",
       "      <th>relevant_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>MO</td>\n",
       "      <td>@theblaze @realDonaldTrump https://t.co/TY9DlZ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>LA</td>\n",
       "      <td>@BarackObama \\n@FBI\\n@LORETTALYNCH \\nALL IN CO...</td>\n",
       "      <td>collusion together</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>MO</td>\n",
       "      <td>@theblaze @realDonaldTrump https://t.co/n050DB...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AU</td>\n",
       "      <td>None</td>\n",
       "      <td>@HillaryClinton he will do in one year all the...</td>\n",
       "      <td>one year things done eight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>MD</td>\n",
       "      <td>#CNN #newday clear #Trump deliberately throwin...</td>\n",
       "      <td>clear deliberately throwing race 2007 knew des...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>US</td>\n",
       "      <td>CA</td>\n",
       "      <td>@realDonaldTrump, you wouldn't recognize a lie...</td>\n",
       "      <td>recognize lie came mouth continually</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GB</td>\n",
       "      <td>None</td>\n",
       "      <td>#Trump2016 #TrumpPence16 #MakeAmericaGreatAgai...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>US</td>\n",
       "      <td>NJ</td>\n",
       "      <td>\"Kid, you know, suing someone? Thats the most ...</td>\n",
       "      <td>kid know suing someone thats beautiful thing 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>US</td>\n",
       "      <td>TX</td>\n",
       "      <td>@HillaryClinton you ARE the co-founder of ISIS...</td>\n",
       "      <td>co founder isis crooked evil lying witch live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AU</td>\n",
       "      <td>None</td>\n",
       "      <td>@Geraldanthro @NeilTurner_ @realDonaldTrump wa...</td>\n",
       "      <td>want comparison try maimed vets pre amp post i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>US</td>\n",
       "      <td>MD</td>\n",
       "      <td>@mike4193496 @realDonaldTrump I TOTALLY CONCUR...</td>\n",
       "      <td>totally concur election cra cra n corruption g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>US</td>\n",
       "      <td>MI</td>\n",
       "      <td>@realDonaldTrump @elsolarverde What issues? Yo...</td>\n",
       "      <td>issues idiot claim founded isis trump go hell ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>US</td>\n",
       "      <td>KY</td>\n",
       "      <td>Can't stand @HillaryClinton or @realDonaldTrum...</td>\n",
       "      <td>can't stand look win settle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>US</td>\n",
       "      <td>IA</td>\n",
       "      <td>@CribBoss @WesSmith123 @realDonaldTrump why is...</td>\n",
       "      <td>rape allegations getting attention cause seemi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>AU</td>\n",
       "      <td>None</td>\n",
       "      <td>@HillaryClinton @TeamUSA @realDonaldTrump was ...</td>\n",
       "      <td>stole white house furniture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>US</td>\n",
       "      <td>CO</td>\n",
       "      <td>GOP pleading w Trump \"Just control your behavi...</td>\n",
       "      <td>gop pleading w trump control behavior weeks wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>US</td>\n",
       "      <td>TX</td>\n",
       "      <td>@HillaryClinton ISIS co-founder Hillary Clinto...</td>\n",
       "      <td>isis co founder hillary clinton obama also dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>IT</td>\n",
       "      <td>None</td>\n",
       "      <td>Come to Jesus meeting!!!! What on earth is tha...</td>\n",
       "      <td>come jesus meeting earth supposed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>US</td>\n",
       "      <td>FL</td>\n",
       "      <td>@sherrilee7 @seanhannity @HillaryClinton #Hann...</td>\n",
       "      <td>think disbarred ignorant mr hamburg dishonest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ID</td>\n",
       "      <td>None</td>\n",
       "      <td>@realDonaldTrump stop worrying about the MSM l...</td>\n",
       "      <td>stop worrying msm lies focus econ amp imm amp ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>PK</td>\n",
       "      <td>None</td>\n",
       "      <td>Is it true? US officially selling arms to ISIS...</td>\n",
       "      <td>true us officially selling arms isis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>US</td>\n",
       "      <td>TX</td>\n",
       "      <td>@HFA @HillaryClinton wake up Dems Hillary is c...</td>\n",
       "      <td>wake dems hillary cofounder isis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>AU</td>\n",
       "      <td>None</td>\n",
       "      <td>@HillaryClinton @TeamUSA @realDonaldTrump oh h...</td>\n",
       "      <td>oh go channeling damn husband</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>US</td>\n",
       "      <td>PA</td>\n",
       "      <td>#morningjoe Have to wonder about the mentality...</td>\n",
       "      <td>wonder mentality members laughed amp clapped a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>US</td>\n",
       "      <td>FL</td>\n",
       "      <td>I don't hate Democrats. I hate the evil ideolo...</td>\n",
       "      <td>hate democrats hate evil ideology abortion imm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   country state                                               text  \\\n",
       "0       US    MO  @theblaze @realDonaldTrump https://t.co/TY9DlZ...   \n",
       "1       US    LA  @BarackObama \\n@FBI\\n@LORETTALYNCH \\nALL IN CO...   \n",
       "2       US    MO  @theblaze @realDonaldTrump https://t.co/n050DB...   \n",
       "3       AU  None  @HillaryClinton he will do in one year all the...   \n",
       "4       US    MD  #CNN #newday clear #Trump deliberately throwin...   \n",
       "5       US    CA  @realDonaldTrump, you wouldn't recognize a lie...   \n",
       "6       GB  None  #Trump2016 #TrumpPence16 #MakeAmericaGreatAgai...   \n",
       "7       US    NJ  \"Kid, you know, suing someone? Thats the most ...   \n",
       "8       US    TX  @HillaryClinton you ARE the co-founder of ISIS...   \n",
       "9       AU  None  @Geraldanthro @NeilTurner_ @realDonaldTrump wa...   \n",
       "10      US    MD  @mike4193496 @realDonaldTrump I TOTALLY CONCUR...   \n",
       "11      US    MI  @realDonaldTrump @elsolarverde What issues? Yo...   \n",
       "12      US    KY  Can't stand @HillaryClinton or @realDonaldTrum...   \n",
       "13      US    IA  @CribBoss @WesSmith123 @realDonaldTrump why is...   \n",
       "14      AU  None  @HillaryClinton @TeamUSA @realDonaldTrump was ...   \n",
       "15      US    CO  GOP pleading w Trump \"Just control your behavi...   \n",
       "16      US    TX  @HillaryClinton ISIS co-founder Hillary Clinto...   \n",
       "17      IT  None  Come to Jesus meeting!!!! What on earth is tha...   \n",
       "18      US    FL  @sherrilee7 @seanhannity @HillaryClinton #Hann...   \n",
       "19      ID  None  @realDonaldTrump stop worrying about the MSM l...   \n",
       "20      PK  None  Is it true? US officially selling arms to ISIS...   \n",
       "21      US    TX  @HFA @HillaryClinton wake up Dems Hillary is c...   \n",
       "22      AU  None  @HillaryClinton @TeamUSA @realDonaldTrump oh h...   \n",
       "23      US    PA  #morningjoe Have to wonder about the mentality...   \n",
       "24      US    FL  I don't hate Democrats. I hate the evil ideolo...   \n",
       "\n",
       "                                      relevant_tokens  \n",
       "0                                                      \n",
       "1                                  collusion together  \n",
       "2                                                      \n",
       "3                          one year things done eight  \n",
       "4   clear deliberately throwing race 2007 knew des...  \n",
       "5                recognize lie came mouth continually  \n",
       "6                                                      \n",
       "7   kid know suing someone thats beautiful thing 1...  \n",
       "8       co founder isis crooked evil lying witch live  \n",
       "9   want comparison try maimed vets pre amp post i...  \n",
       "10  totally concur election cra cra n corruption g...  \n",
       "11  issues idiot claim founded isis trump go hell ...  \n",
       "12                        can't stand look win settle  \n",
       "13  rape allegations getting attention cause seemi...  \n",
       "14                        stole white house furniture  \n",
       "15  gop pleading w trump control behavior weeks wa...  \n",
       "16  isis co founder hillary clinton obama also dev...  \n",
       "17                  come jesus meeting earth supposed  \n",
       "18      think disbarred ignorant mr hamburg dishonest  \n",
       "19  stop worrying msm lies focus econ amp imm amp ...  \n",
       "20               true us officially selling arms isis  \n",
       "21                   wake dems hillary cofounder isis  \n",
       "22                      oh go channeling damn husband  \n",
       "23  wonder mentality members laughed amp clapped a...  \n",
       "24  hate democrats hate evil ideology abortion imm...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['country','state','text','relevant_tokens']].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>country</th>\n",
       "      <th>state</th>\n",
       "      <th>city</th>\n",
       "      <th>text</th>\n",
       "      <th>mentions</th>\n",
       "      <th>links</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>relevant_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>und</td>\n",
       "      <td>US</td>\n",
       "      <td>MO</td>\n",
       "      <td>Frontenac</td>\n",
       "      <td>@theblaze @realDonaldTrump https://t.co/TY9DlZ...</td>\n",
       "      <td>[TheBlaze, Donald J. Trump]</td>\n",
       "      <td>[https://t.co/TY9DlZ584c]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en</td>\n",
       "      <td>US</td>\n",
       "      <td>LA</td>\n",
       "      <td>Baton Rouge</td>\n",
       "      <td>@BarackObama \\n@FBI\\n@LORETTALYNCH \\nALL IN CO...</td>\n",
       "      <td>[Barack Obama, FBI, AG Loretta Lynch, Donald J...</td>\n",
       "      <td>[https://t.co/5GMNZq40V3]</td>\n",
       "      <td>[#NOJUSTICE, #TrumpPence]</td>\n",
       "      <td>collusion together</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>und</td>\n",
       "      <td>US</td>\n",
       "      <td>MO</td>\n",
       "      <td>Frontenac</td>\n",
       "      <td>@theblaze @realDonaldTrump https://t.co/n050DB...</td>\n",
       "      <td>[TheBlaze, Donald J. Trump]</td>\n",
       "      <td>[https://t.co/n050DBSpv0]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en</td>\n",
       "      <td>AU</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>@HillaryClinton he will do in one year all the...</td>\n",
       "      <td>[Hillary Clinton]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>one year things done eight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en</td>\n",
       "      <td>US</td>\n",
       "      <td>MD</td>\n",
       "      <td>Baltimore</td>\n",
       "      <td>#CNN #newday clear #Trump deliberately throwin...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[#CNN, #newday, #Trump, #ISIS]</td>\n",
       "      <td>clear deliberately throwing race 2007 knew des...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang country state         city  \\\n",
       "0  und      US    MO    Frontenac   \n",
       "1   en      US    LA  Baton Rouge   \n",
       "2  und      US    MO    Frontenac   \n",
       "3   en      AU  None         None   \n",
       "4   en      US    MD    Baltimore   \n",
       "\n",
       "                                                text  \\\n",
       "0  @theblaze @realDonaldTrump https://t.co/TY9DlZ...   \n",
       "1  @BarackObama \\n@FBI\\n@LORETTALYNCH \\nALL IN CO...   \n",
       "2  @theblaze @realDonaldTrump https://t.co/n050DB...   \n",
       "3  @HillaryClinton he will do in one year all the...   \n",
       "4  #CNN #newday clear #Trump deliberately throwin...   \n",
       "\n",
       "                                            mentions  \\\n",
       "0                        [TheBlaze, Donald J. Trump]   \n",
       "1  [Barack Obama, FBI, AG Loretta Lynch, Donald J...   \n",
       "2                        [TheBlaze, Donald J. Trump]   \n",
       "3                                  [Hillary Clinton]   \n",
       "4                                                 []   \n",
       "\n",
       "                       links                        hashtags  \\\n",
       "0  [https://t.co/TY9DlZ584c]                                   \n",
       "1  [https://t.co/5GMNZq40V3]       [#NOJUSTICE, #TrumpPence]   \n",
       "2  [https://t.co/n050DBSpv0]                                   \n",
       "3                                                              \n",
       "4                             [#CNN, #newday, #Trump, #ISIS]   \n",
       "\n",
       "                                     relevant_tokens  \n",
       "0                                                     \n",
       "1                                 collusion together  \n",
       "2                                                     \n",
       "3                         one year things done eight  \n",
       "4  clear deliberately throwing race 2007 knew des...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)\n",
    "#df.to_excel('geomapped_tweets.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 collusion\n",
      "1 together\n",
      "2 done\n",
      "3 eight\n",
      "4 one\n",
      "5 thing\n",
      "6 year\n",
      "7 2007\n",
      "8 clear\n",
      "9 deliberately\n",
      "10 destabilization\n"
     ]
    }
   ],
   "source": [
    "#create a dictionary\n",
    "tokens = df['text'].apply(lambda text: raw_tokens(text))\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove extemes\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Topic: 0 Word: 0.011*\"lol\" + 0.009*\"go\" + 0.007*\"trump\" + 0.007*\"yep\" + 0.007*\"right\" + 0.006*\"never\" + 0.005*\"amp\" + 0.005*\"u\" + 0.005*\"hillary\" + 0.004*\"wall\"\n",
      "Topic: 1 Word: 0.011*\"trump\" + 0.009*\"like\" + 0.009*\"lie\" + 0.008*\"hillary\" + 0.007*\"know\" + 0.007*\"vote\" + 0.007*\"u\" + 0.006*\"one\" + 0.006*\"look\" + 0.006*\"think\"\n",
      "Topic: 2 Word: 0.021*\"deplorable\" + 0.011*\"l\" + 0.011*\"tax\" + 0.009*\"check\" + 0.009*\"tweet\" + 0.007*\"return\" + 0.006*\"release\" + 0.006*\"trump\" + 0.005*\"wow\" + 0.005*\"u\"\n",
      "Topic: 3 Word: 0.081*\"0\" + 0.020*\"2016\" + 0.019*\"f\" + 0.018*\"fine\" + 0.017*\"today\" + 0.017*\"pressure\" + 0.017*\"30\" + 0.016*\"wind\" + 0.016*\"temp\" + 0.016*\"ky\"\n",
      "Topic: 4 Word: 0.009*\"liar\" + 0.008*\"truth\" + 0.006*\"trump\" + 0.006*\"amp\" + 0.005*\"question\" + 0.005*\"u\" + 0.005*\"lie\" + 0.005*\"hillary\" + 0.004*\"medium\" + 0.004*\"like\"\n",
      "Topic: 5 Word: 0.022*\"de\" + 0.015*\"la\" + 0.014*\"que\" + 0.012*\"love\" + 0.012*\"true\" + 0.010*\"el\" + 0.010*\"en\" + 0.008*\"un\" + 0.007*\"con\" + 0.006*\"se\"\n",
      "Topic: 6 Word: 0.010*\"amp\" + 0.010*\"pneumonia\" + 0.008*\"trump\" + 0.008*\"u\" + 0.008*\"get\" + 0.008*\"vote\" + 0.007*\"agree\" + 0.007*\"please\" + 0.006*\"american\" + 0.006*\"white\"\n",
      "Topic: 7 Word: 0.006*\"clinton\" + 0.006*\"amp\" + 0.006*\"hillary\" + 0.006*\"god\" + 0.005*\"trump\" + 0.005*\"u\" + 0.005*\"email\" + 0.005*\"money\" + 0.004*\"foundation\" + 0.004*\"pay\"\n",
      "Topic: 8 Word: 0.015*\"e\" + 0.013*\"yes\" + 0.009*\"you'r\" + 0.009*\"great\" + 0.007*\"president\" + 0.007*\"trump\" + 0.006*\"america\" + 0.005*\"make\" + 0.005*\"said\" + 0.005*\"amp\"\n",
      "Topic: 9 Word: 0.012*\"basket\" + 0.008*\"fuck\" + 0.008*\"deplorables\" + 0.007*\"trump\" + 0.006*\"say\" + 0.006*\"racist\" + 0.005*\"amp\" + 0.004*\"u\" + 0.004*\"hillary\" + 0.004*\"know\"\n"
     ]
    }
   ],
   "source": [
    "#bag of words approach\n",
    "#bow_corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "#bow_doc_4310 = bow_corpus[4310]\n",
    "#for i in range(len(bow_doc_4310)):\n",
    "#    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "#                                               dictionary[bow_doc_4310[i][0]], \n",
    "#bow_doc_4310[i][1]))\n",
    "#from gensim import corpora, models    \n",
    "#lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)\n",
    "#for idx, topic in lda_model.print_topics(-1):\n",
    "#    print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
    "\n",
    "##TF_IDF approach\n",
    "bow_corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break\n",
    "import gensim\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lda_model_tfidf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-48cc4a3f42ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Evaluating model performance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda_model_tfidf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbow_corpus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m310\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nScore: {}\\t \\nTopic: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlda_model_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_topic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lda_model_tfidf' is not defined"
     ]
    }
   ],
   "source": [
    "#Evaluating model performance\n",
    "for index, score in sorted(lda_model_tfidf[bow_corpus[310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))\n",
    "    \n",
    "    \n",
    "#testing model on unseen data\n",
    "new_tweet = 'The art of war is not something to be celebrated. #realshit'\n",
    "bow_vector = dictionary.doc2bow(raw_tokens(new_tweet))\n",
    "for index, score in sorted(lda_model_tfidf[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model_tfidf.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for i in df['text']:\n",
    "    \n",
    "    allwords_stemmed = tokenize_and_stem(i) #for each item in 'synopses', tokenize/stem\n",
    "    totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)\n",
    "    \n",
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "print ('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')\n",
    "\n",
    "print (vocab_frame.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.9, max_features=100000,\n",
    "                                 min_df=0.1, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(text) #fit the vectorizer to synopses\n",
    "print(tfidf_matrix.shape)\n",
    "\n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)\n",
    "print\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "num_clusters = 10\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "%time km.fit(tfidf_matrix)\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "dframe = { 'country': country, 'text': text, 'cluster': clusters,'tokens': allwords_stemmed }\n",
    "frame = pd.DataFrame(dframe, index = [clusters] , columns = ['country', 'text' 'cluster', 'tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :]5: #replace 6 with n words per cluster\n",
    "        print(' %s' % vocab_frame.ix[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "    print(\"Cluster %d tokens:\" % i, end='')\n",
    "    for title in frame.ix[i]['tokens'].values.tolist():\n",
    "        print(' %s,' % title, end='')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\cliff\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\cliff\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\cliff\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\cliff\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\cliff\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\cliff\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\cliff\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\cliff\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\cliff\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\cliff\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\cliff\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\cliff\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\omw.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\cliff\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\cliff\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\cliff\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\cliff\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\cliff\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-957ee4207649>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmovie_reviews\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'popular'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\nltk\\downloader.py\u001b[0m in \u001b[0;36mdownload\u001b[1;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error)\u001b[0m\n\u001b[0;32m    668\u001b[0m                                     subsequent_indent=prefix+prefix2+' '*4))\n\u001b[0;32m    669\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mmsg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mincr_download\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo_or_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    671\u001b[0m                 \u001b[1;31m# Error messages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mErrorMessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\nltk\\downloader.py\u001b[0m in \u001b[0;36mincr_download\u001b[1;34m(self, info_or_id, download_dir, force)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCollection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mStartCollectionMessage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mmsg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mincr_download\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mFinishCollectionMessage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\nltk\\downloader.py\u001b[0m in \u001b[0;36mincr_download\u001b[1;34m(self, info_or_id, download_dir, force)\u001b[0m\n\u001b[0;32m    533\u001b[0m         \u001b[1;31m# If they gave us a list of ids, then download each one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo_or_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 535\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mmsg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_download_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo_or_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    536\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\nltk\\downloader.py\u001b[0m in \u001b[0;36m_download_list\u001b[1;34m(self, items, download_dir, force)\u001b[0m\n\u001b[0;32m    576\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m                 \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpackages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnum_packages\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 578\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mmsg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mincr_download\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    579\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mProgressMessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m                     \u001b[1;32myield\u001b[0m \u001b[0mProgressMessage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprogress\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogress\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\nltk\\downloader.py\u001b[0m in \u001b[0;36mincr_download\u001b[1;34m(self, info_or_id, download_dir, force)\u001b[0m\n\u001b[0;32m    553\u001b[0m         \u001b[1;31m# Handle Packages (delegate to a helper function).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mmsg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_download_package\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    556\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\nltk\\downloader.py\u001b[0m in \u001b[0;36m_download_package\u001b[1;34m(self, info, download_dir, force)\u001b[0m\n\u001b[0;32m    622\u001b[0m                 \u001b[0mnum_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 624\u001b[1;33m                     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 16k blocks.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    625\u001b[0m                     \u001b[0moutfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    447\u001b[0m             \u001b[1;31m# Amount is given, implement using readinto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m             \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\http\\client.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    491\u001b[0m         \u001b[1;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[1;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1000\u001b[0m                   \u001b[1;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1002\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1003\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m    863\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Read on closed or unwrapped SSL socket.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 865\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    866\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m    623\u001b[0m         \"\"\"\n\u001b[0;32m    624\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m             \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m             \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Download movie rewiews from nltk (tagged with pos/neg sentiment) \n",
    "from nltk.corpus import movie_reviews as mr\n",
    "from collections import defaultdict\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pos/cv000_29590.txt', 'pos/cv001_18431.txt', 'pos/cv002_15918.txt', 'pos/cv003_11664.txt', 'pos/cv004_11636.txt', 'pos/cv005_29443.txt', 'pos/cv006_15448.txt', 'pos/cv007_4968.txt', 'pos/cv008_29435.txt', 'pos/cv009_29592.txt']\n",
      "['neg/cv000_29416.txt', 'neg/cv001_19502.txt', 'neg/cv002_17424.txt', 'neg/cv003_12683.txt', 'neg/cv004_12641.txt', 'neg/cv005_29357.txt', 'neg/cv006_17022.txt', 'neg/cv007_4992.txt', 'neg/cv008_29326.txt', 'neg/cv009_29417.txt']\n"
     ]
    }
   ],
   "source": [
    "# create list of movie reviews by sentiment\n",
    "documents = defaultdict(list)\n",
    "for i in mr.fileids():\n",
    "    documents[i.split('/')[0]].append(i)\n",
    "\n",
    "print(documents['pos'][:10]) # first ten pos reviews.\n",
    "print(documents['neg'][:10]) # neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract all words from movie reviews with given sentiment\n",
    "import string\n",
    "stop = stopwords.words('english')\n",
    "documents = [([w for w in mr.words(i) if w.lower() not in stop and w.lower() not in string.punctuation], i.split('/')[0]) for i in mr.fileids()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from nltk.classify import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input: list of training docs, output: all words in those docs.\n",
    "def get_words_in_training(docs):\n",
    "    all_words = []\n",
    "    for (words, sentiment) in docs:\n",
    "        all_words.extend(words)\n",
    "    return all_words\n",
    "\n",
    "# Input: list of words, output: features\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features\n",
    "\n",
    "word_features = get_word_features(get_words_in_training(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input: list of tweets, output: feature list for that tweet\n",
    "def extract_features(tweets):\n",
    "    for tweet in tweets:\n",
    "        document_words = tweet.split(' ')\n",
    "        features = {}\n",
    "        for word in word_features:\n",
    "            features['contains(%s)' % word] = (word in tweet)\n",
    "        return features\n",
    "\n",
    "# get training set\n",
    "training_set = nltk.classify.apply_features(extract_features, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-d94bce1ba7ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNaiveBayesClassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\classify\\naivebayes.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(cls, labeled_featuresets, estimator)\u001b[0m\n\u001b[0;32m    196\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m                 \u001b[1;31m# Increment freq(fval|label, fname)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m                 \u001b[0mfeature_freqdist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfval\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m                 \u001b[1;31m# Record that fname can take the value fval.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m                 \u001b[0mfeature_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\probability.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, val)\u001b[0m\n\u001b[0;32m    130\u001b[0m         \"\"\"\n\u001b[0;32m    131\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_N\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__delitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: clean up features list, i.e. throw away useless features and words that don't show up in the tweets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
